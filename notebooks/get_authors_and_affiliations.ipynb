{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import requests\n",
    "import random\n",
    "import lxml\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "UserAgent = [\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\"\n",
    "        ]\n",
    "\n",
    "\n",
    "def requestHeader(url):\n",
    "    # Build request headers\n",
    "    headers = {\n",
    "            'User-Agent':random.choice(UserAgent),\n",
    "            'Referer': url,\n",
    "            'Connection':'keep-alive'\n",
    "            }\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81352af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../secret/elsevier_api.txt') as f:\n",
    "    api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_csv('../data/240107_combined_sem_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['publisher'] = meta_df['doi'].apply(lambda x: x.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df.drop_duplicates('doi')[['doi', 'publisher']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376afb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid waiting for 1 second on Nature Portfolio requests by randomizing order\n",
    "np.random.seed(50)\n",
    "\n",
    "meta_df = meta_df.sample(frac=1).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055486e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_dict = {1:'January', 2:'February', 3:'March', 4:'April', 5:'May', 6:'June', \n",
    "            7:'July', 8:'August', 9:'September', 10:'October', 11:'November', 12:'December'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frontiers_aff(doi):\n",
    "    author_array = []\n",
    "    aff_array = []\n",
    "    pub_date_array = []\n",
    "    url = 'https://doi.org/' + doi\n",
    "    response = requests.get(url, headers= requestHeader(url))\n",
    "\n",
    "    file = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    author_names = file.find('div', {'class':'authors'}).find_all(text=True)\n",
    "    sup_text_array = [sup.text for sup in file.find('div', {'class':'authors'}).find_all('sup')]\n",
    "        \n",
    "    author_names = np.array([x.replace('*', '').replace('â€ ', '').strip() for x in author_names])\n",
    "    author_names = author_names[author_names != '']\n",
    "    author_names = author_names[[author_name not in sup_text_array for author_name in author_names]]\n",
    "\n",
    "    ###### Authors\n",
    "    if len(sup_text_array) > 0:\n",
    "        for full_name, sup_text in zip(author_names, sup_text_array):\n",
    "            for sup in sup_text.split(','):\n",
    "                author_array.append(pd.DataFrame({'doi':[doi], 'full_name':[full_name], 'script':[sup]}))\n",
    "    else:\n",
    "        for full_name in author_names:\n",
    "            author_array.append(pd.DataFrame({'doi':[doi], 'full_name':[full_name], 'script':['0']}))\n",
    "\n",
    "    ######### Affiliations            \n",
    "    aff_objects = file.find('ul', {'class':'notes'}).find_all('li')\n",
    "    for aff_object in aff_objects:\n",
    "        if aff_object.find('sup'):\n",
    "            sup = aff_object.find('sup').text\n",
    "            aff_text = aff_object.text.replace(sup, '', 1)\n",
    "            aff_array.append(pd.DataFrame({'doi':[doi], 'affiliation':[aff_text], 'script':[sup]}))\n",
    "        else:\n",
    "            aff_text = aff_object.text\n",
    "            aff_array.append(pd.DataFrame({'doi':[doi], 'affiliation':[aff_text], 'script':['0']}))\n",
    "\n",
    "    ####### Handling dates\n",
    "    full_date_text = file.find('p', {'id':'timestamps'}).text\n",
    "    for date_text in full_date_text.split(';'):\n",
    "        date_text = date_text.strip()\n",
    "        action = date_text.split(':')[0].strip()\n",
    "        date_str = date_text.split(':')[1].strip().replace('.', '')\n",
    "        pub_date_array.append(pd.DataFrame({'doi':[doi], 'action':[action], 'date':[date_str]}))\n",
    "            \n",
    "    author_df = pd.DataFrame()\n",
    "    aff_df = pd.DataFrame()\n",
    "    pub_date_df = pd.DataFrame()\n",
    "    \n",
    "    if len(author_array) > 0:\n",
    "        author_df = pd.concat(author_array)\n",
    "    if len(aff_array) > 0:\n",
    "        aff_df = pd.concat(aff_array)\n",
    "    if len(pub_date_array) > 0:\n",
    "        pub_date_df = pd.concat(pub_date_array)\n",
    "\n",
    "    file.decompose()\n",
    "        \n",
    "    return author_df, aff_df, pub_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe840239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plos_aff(doi):\n",
    "    author_array = []\n",
    "    aff_array = []\n",
    "    pub_date_array = []\n",
    "\n",
    "    url = 'https://doi.org/' + doi\n",
    "    response = requests.get(url, headers= requestHeader(url))\n",
    "\n",
    "    file = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    ######### Authors & Affiliations\n",
    "    author_objects = file.find_all('a', {'class':'author-name'})\n",
    "        \n",
    "    author_count = 0\n",
    "    for author_object in author_objects:\n",
    "        author_count = author_object['data-author-id']\n",
    "        full_name = author_object.text.replace('\\n', '').strip(' ,')\n",
    "        sup = str(author_count)\n",
    "        author_array.append(pd.DataFrame({'doi':[doi], 'full_name':[full_name], 'script':[sup]}))\n",
    "        try:\n",
    "            aff_text = file.find('p', {'id':\"authAffiliations-\" + author_count}).text.replace('\\n', '').strip(' ,')\n",
    "        except:\n",
    "            aff_text = ''\n",
    "        aff_array.append(pd.DataFrame({'doi':[doi], 'affiliation':[aff_text], 'script':[sup]}))\n",
    "\n",
    "    ########## Handling dates\n",
    "    article_info = file.find(class_='articleinfo')\n",
    "    for p in article_info.find_all('p'):\n",
    "        text = p.text\n",
    "        if text.startswith('Received'):\n",
    "            true_text = text.split(';')\n",
    "        else:\n",
    "            true_text = [text]\n",
    "                \n",
    "        for text_ele in true_text:\n",
    "            ident = text_ele.split(':')[0]\n",
    "            value = text_ele.split(':')[1].strip()\n",
    "            pub_date_array.append(pd.DataFrame({'doi':[doi], 'action':[ident], 'date':[value]}))\n",
    "\n",
    "    author_df = pd.DataFrame()\n",
    "    aff_df = pd.DataFrame()\n",
    "    pub_date_df = pd.DataFrame()\n",
    "    \n",
    "    if len(author_array) > 0:\n",
    "        author_df = pd.concat(author_array)\n",
    "    if len(aff_array) > 0:\n",
    "        aff_df = pd.concat(aff_array)\n",
    "    if len(pub_date_array) > 0:\n",
    "        pub_date_df = pd.concat(pub_date_array)\n",
    "\n",
    "    file.decompose()\n",
    "        \n",
    "    return author_df, aff_df, pub_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nature_aff(doi):\n",
    "    author_array = []\n",
    "    aff_array = []\n",
    "    pub_date_array = []\n",
    "\n",
    "    url = 'https://doi.org/' + doi\n",
    "    response = requests.get(url, headers= requestHeader(url))\n",
    "\n",
    "    file = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    ######### Authors & Affiliations\n",
    "    author_objects = json.loads(file.find_all('script', {'type':'application/ld+json'})[0].text)['mainEntity']['author']\n",
    "        \n",
    "    author_count = 0\n",
    "    for author_object in author_objects:\n",
    "        full_name = author_object['name']\n",
    "        sup = str(author_count)\n",
    "        author_array.append(pd.DataFrame({'doi':[doi], 'full_name':[full_name], 'script':[sup]}))\n",
    "            \n",
    "        for affiliation_object in author_object['affiliation']:\n",
    "            aff_text = affiliation_object['address']['name']\n",
    "            aff_array.append(pd.DataFrame({'doi':[doi], 'affiliation':[aff_text], 'script':[sup]}))\n",
    "        author_count += 1\n",
    "            \n",
    "    ######### Handling dates & pub dates\n",
    "    \n",
    "    for date_object in file.find_all('li', {'class':'c-bibliographic-information__list-item'}):\n",
    "        date_text = date_object.find('p').text\n",
    "        if 'DOI' in date_text:\n",
    "            pass\n",
    "        else:\n",
    "            action = date_text.split(':')[0]\n",
    "            date_str = date_text.split(':')[-1].strip()\n",
    "            pub_date_array.append(pd.DataFrame({'doi':[doi], 'action':[action], 'date':[date_str]}))\n",
    "\n",
    "    author_df = pd.DataFrame()\n",
    "    aff_df = pd.DataFrame()\n",
    "    pub_date_df = pd.DataFrame()\n",
    "    \n",
    "    if len(author_array) > 0:\n",
    "        author_df = pd.concat(author_array)\n",
    "    if len(aff_array) > 0:\n",
    "        aff_df = pd.concat(aff_array)\n",
    "    if len(pub_date_array) > 0:\n",
    "        pub_date_df = pd.concat(pub_date_array)\n",
    "\n",
    "    file.decompose()\n",
    "        \n",
    "    return author_df, aff_df, pub_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elsevier_aff(doi, api_key):\n",
    "    author_array = []\n",
    "    aff_array = []\n",
    "    pub_date_array = []\n",
    "\n",
    "    url = 'https://api.elsevier.com/content/article/doi/' + doi + '?APIKey=' + api_key + '&view=FULL'\n",
    "    response = requests.get(url, headers= requestHeader(url))\n",
    "\n",
    "    file = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    ######### Authors\n",
    "\n",
    "    # Check for editors.\n",
    "\n",
    "    editor_ids = []\n",
    "    editor_aff_ids = []\n",
    "    try:\n",
    "\n",
    "        for editor_object in file.find_all('ce:editors'):\n",
    "            editor_author_objects = editor_object.find_all('ce:author')\n",
    "            editor_aff_objects = editor_object.find_all('ce:affiliation')\n",
    "            for editor_author_object in editor_author_objects:\n",
    "                editor_ids.append(editor_author_object['author-id'])\n",
    "            for editor_aff_object in editor_aff_objects:\n",
    "                editor_aff_ids.append(editor_aff_object['affiliation-id'])\n",
    "    except:\n",
    "        editor_ids = []\n",
    "        editor_aff_ids = []\n",
    "        \n",
    "    for author_object in file.find_all('ce:author'):\n",
    "        pipe = False\n",
    "        try:\n",
    "            pipe = (author_object['author-id'] not in editor_ids)\n",
    "        except:\n",
    "            pipe = True\n",
    "            \n",
    "        if pipe:\n",
    "            # given_name\n",
    "            if author_object.find('ce:given-name'):\n",
    "                given_name = author_object.find('ce:given-name').text\n",
    "            else:\n",
    "                given_name = ''\n",
    "    \n",
    "            # surname\n",
    "            if author_object.find('ce:surname'):\n",
    "                surname = author_object.find('ce:surname').text\n",
    "            else:\n",
    "                surname = ''\n",
    "    \n",
    "            full_name = given_name + ' ' + surname\n",
    "    \n",
    "                # script\n",
    "            if author_object.find('ce:sup'):\n",
    "                sup_objects = author_object.find_all('ce:sup')\n",
    "                for sup_object in sup_objects:\n",
    "                    sup = sup_object.text\n",
    "                    author_array.append(pd.DataFrame({'doi':[doi], 'full_name':[full_name], 'script':[sup]}))    \n",
    "            else:\n",
    "                sup = ''\n",
    "                author_array.append(pd.DataFrame({'doi':[doi], 'full_name':[full_name], 'script':[sup]}))\n",
    "\n",
    "    ######### Affiliations\n",
    "    for affiliation_object in file.find_all('ce:affiliation'):\n",
    "\n",
    "        pipe = False\n",
    "        try:\n",
    "            pipe = (affiliation_object['affiliation-id'] not in editor_aff_ids)\n",
    "        except:\n",
    "            pipe = True\n",
    "            \n",
    "        if pipe:\n",
    "\n",
    "            # aff text\n",
    "            if affiliation_object.find('ce:textfn'):\n",
    "                aff_text = affiliation_object.find('ce:textfn').text\n",
    "            else:\n",
    "                aff_text = ''\n",
    "    \n",
    "            # script\n",
    "            if affiliation_object.find('ce:label'):\n",
    "                sup = affiliation_object.find('ce:label').text\n",
    "            else:\n",
    "                sup = ''\n",
    "    \n",
    "            aff_array.append(pd.DataFrame({'doi':[doi], 'affiliation':[aff_text], 'script':[sup]}))\n",
    "\n",
    "    ######### Handling dates\n",
    "    for date_object in file.find_all('ce:date-received'):\n",
    "        pub_date_array.append(pd.DataFrame({'doi':[doi], 'action':['received'], 'date':[date_object['year'] + '-' + \\\n",
    "                                                                                        date_object['month'] + '-' + \\\n",
    "                                                                                       date_object['day']]}))\n",
    "\n",
    "    for date_object in file.find_all('ce:date-revised'):\n",
    "        pub_date_array.append(pd.DataFrame({'doi':[doi], 'action':['revised'], 'date':[date_object['year'] + '-' + \\\n",
    "                                                                                        date_object['month'] + '-' + \\\n",
    "                                                                                       date_object['day']]}))\n",
    "\n",
    "    for date_object in file.find_all('ce:date-accepted'):\n",
    "        pub_date_array.append(pd.DataFrame({'doi':[doi], 'action':['accepted'], 'date':[date_object['year'] + '-' + \\\n",
    "                                                                                        date_object['month'] + '-' + \\\n",
    "                                                                                       date_object['day']]}))\n",
    "\n",
    "\n",
    "        ######### Publication dates\n",
    "    for date_object in file.find_all('xocs:available-online-date'):\n",
    "        pub_date_array.append(pd.DataFrame({'doi':[doi], 'action':['available online'], 'date':[date_object.text]}))\n",
    "    for date_object in file.find_all('xocs:vor-available-online-date'):\n",
    "        pub_date_array.append(pd.DataFrame({'doi':[doi], 'action':['version of record'], 'date':[date_object.text]}))\n",
    "\n",
    "    author_df = pd.DataFrame()\n",
    "    aff_df = pd.DataFrame()\n",
    "    pub_date_df = pd.DataFrame()\n",
    "    \n",
    "    if len(author_array) > 0:\n",
    "        author_df = pd.concat(author_array)\n",
    "    if len(aff_array) > 0:\n",
    "        aff_df = pd.concat(aff_array)\n",
    "    if len(pub_date_array) > 0:\n",
    "        pub_date_df = pd.concat(pub_date_array)\n",
    "\n",
    "    file.decompose()\n",
    "        \n",
    "    return author_df, aff_df, pub_date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c9858",
   "metadata": {},
   "source": [
    "Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75768837",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df, aff_df, pub_date_df = elsevier_aff('10.1016/j.microc.2015.12.017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a669177",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df, aff_df, pub_date_df = plos_aff('10.1371/journal.pone.0287514')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df, aff_df, pub_date_df = nature_aff('10.1038/s41598-024-52705-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df, aff_df, pub_date_df = frontiers_aff('10.3389/fmats.2020.563233')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1df976",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df, aff_df, pub_date_df = elsevier_aff('10.1016/j.ijhydene.2022.10.031')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df, aff_df, pub_date_df = elsevier_aff('10.1016/j.ijhydene.2023.03.046')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ea766",
   "metadata": {},
   "source": [
    "Collect all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_array = []\n",
    "aff_array = []\n",
    "pub_date_array = []\n",
    "\n",
    "for index, row in meta_df.iterrows():\n",
    "    doi = row['doi']\n",
    "    publisher = row['publisher']\n",
    "\n",
    "    author_df = pd.DataFrame()\n",
    "    aff_df = pd.DataFrame()\n",
    "    pub_date_df = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        if publisher == '10.1016':\n",
    "            author_df, aff_df, pub_date_df = elsevier_aff(doi)\n",
    "        elif publisher == '10.1371':\n",
    "            author_df, aff_df, pub_date_df = plos_aff(doi)\n",
    "        elif publisher == '10.1038':\n",
    "            author_df, aff_df, pub_date_df = nature_aff(doi)\n",
    "        elif publisher == '10.3389':\n",
    "            author_df, aff_df, pub_date_df = frontiers_aff(doi)\n",
    "    except:\n",
    "        print('Error processing DOI: ' + doi)\n",
    "        with open('../data/affiliations_timeout.txt', 'a+') as f:\n",
    "            f.write(doi + '\\n')\n",
    "            \n",
    "    author_array.append(author_df)\n",
    "    aff_array.append(aff_df)\n",
    "    pub_date_array.append(pub_date_df)\n",
    "\n",
    "    print(str(index) + ' DOIs processed.')\n",
    "\n",
    "    if (index % 1000 == 0) & (index > 0):\n",
    "        author_df = pd.concat(author_array)\n",
    "        aff_df = pd.concat(aff_array)\n",
    "        pub_date_df = pd.concat(pub_date_array)\n",
    "\n",
    "        author_array = []\n",
    "        aff_array = []\n",
    "        pub_date_array = []\n",
    "        \n",
    "        author_df.to_csv('../data/240205_sem_authors_' + str(index) + '.csv', index=False)\n",
    "        aff_df.to_csv('../data/240205_sem_affiliations_' + str(index) + '.csv', index=False)\n",
    "        pub_date_df.to_csv('../data/240205_sem_dates_' + str(index) + '.csv', index=False)\n",
    "        \n",
    "    if index % 10 == 0:\n",
    "        clear_output()\n",
    "        gc.collect()\n",
    "\n",
    "author_df = pd.concat(author_array)\n",
    "aff_df = pd.concat(aff_array)\n",
    "pub_date_df = pd.concat(pub_date_array)\n",
    "\n",
    "author_array = []\n",
    "aff_array = []\n",
    "pub_date_array = []\n",
    "        \n",
    "author_df.to_csv('../data/240205_sem_authors_' + str(index) + '.csv', index=False)\n",
    "aff_df.to_csv('../data/240205_sem_affiliations_' + str(index) + '.csv', index=False)\n",
    "pub_date_df.to_csv('../data/240205_sem_dates_' + str(index) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f1ab1",
   "metadata": {},
   "source": [
    "Collect any that timed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8772ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/affiliations_timeout.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69abe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_array = []\n",
    "aff_array = []\n",
    "pub_date_array = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    doi = row[0]\n",
    "    publisher = doi.split('/')[0]\n",
    "\n",
    "    author_df = pd.DataFrame()\n",
    "    aff_df = pd.DataFrame()\n",
    "    pub_date_df = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        if publisher == '10.1016':\n",
    "            author_df, aff_df, pub_date_df = elsevier_aff(doi)\n",
    "        elif publisher == '10.1371':\n",
    "            author_df, aff_df, pub_date_df = plos_aff(doi)\n",
    "        elif publisher == '10.1038':\n",
    "            author_df, aff_df, pub_date_df = nature_aff(doi)\n",
    "        elif publisher == '10.3389':\n",
    "            author_df, aff_df, pub_date_df = frontiers_aff(doi)\n",
    "    except:\n",
    "        print('Error processing DOI: ' + doi)\n",
    "        with open('../data/affiliations_timeout.txt', 'a+') as f:\n",
    "            f.write(doi + '\\n')\n",
    "            \n",
    "    author_array.append(author_df)\n",
    "    aff_array.append(aff_df)\n",
    "    pub_date_array.append(pub_date_df)\n",
    "\n",
    "    print(str(index) + ' DOIs processed.')\n",
    "\n",
    "    if (index % 1000 == 0) & (index > 0):\n",
    "        author_df = pd.concat(author_array)\n",
    "        aff_df = pd.concat(aff_array)\n",
    "        pub_date_df = pd.concat(pub_date_array)\n",
    "\n",
    "        author_array = []\n",
    "        aff_array = []\n",
    "        pub_date_array = []\n",
    "        \n",
    "        author_df.to_csv('../data/240206_sem_authors_' + str(index) + '.csv', index=False)\n",
    "        aff_df.to_csv('../data/240206_sem_affiliations_' + str(index) + '.csv', index=False)\n",
    "        pub_date_df.to_csv('../data/240206_sem_dates_' + str(index) + '.csv', index=False)\n",
    "        \n",
    "    if index % 10 == 0:\n",
    "        clear_output()\n",
    "        gc.collect()\n",
    "\n",
    "author_df = pd.concat(author_array)\n",
    "aff_df = pd.concat(aff_array)\n",
    "pub_date_df = pd.concat(pub_date_array)\n",
    "\n",
    "author_array = []\n",
    "aff_array = []\n",
    "pub_date_array = []\n",
    "        \n",
    "author_df.to_csv('../data/240206_sem_authors_' + str(index) + '.csv', index=False)\n",
    "aff_df.to_csv('../data/240206_sem_affiliations_' + str(index) + '.csv', index=False)\n",
    "pub_date_df.to_csv('../data/240206_sem_dates_' + str(index) + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
