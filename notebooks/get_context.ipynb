{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('../general/src/')\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "from scipy.stats import fisher_exact\n",
    "pd.options.display.precision = 3\n",
    "pd.options.display.expand_frame_repr = False\n",
    "pd.options.display.max_columns = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import scipy\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UserAgent = [\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\"\n",
    "        ]\n",
    "\n",
    "\n",
    "def requestHeader(url):\n",
    "    # Build request headers\n",
    "    headers = {\n",
    "            'User-Agent':random.choice(UserAgent),\n",
    "            'Referer': url,\n",
    "            'Connection':'keep-alive'\n",
    "            }\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67efa8e",
   "metadata": {},
   "source": [
    "Elsevier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede03611",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../secret/elsevier_api.txt') as f:\n",
    "    api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cafbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sciencedirect journals\n",
    "\n",
    "for journal_name in ['msea']:\n",
    "\n",
    "    text_df = pd.read_csv('../data/tesseract_ocr_sem_batch_231217_' + journal_name + '.csv')\n",
    "\n",
    "    meta_df = pd.read_csv('../data/sem_images_231217_batch.csv')\n",
    "\n",
    "    text_df = pd.merge(meta_df, text_df).copy()\n",
    "\n",
    "    text_df = text_df.dropna(subset=['text']).copy()\n",
    "\n",
    "    text_df['instrument_make'] = np.nan\n",
    "\n",
    "    scope_make_dict = {'TESCAN':'TESCAN',\n",
    "                        'TESCA':'TESCAN',\n",
    "                        'TESC':'TESCAN',\n",
    "                        'VEGA':'TESCAN',\n",
    "                        'VEGA3':'TESCAN',\n",
    "                        'VEGAN':'TESCAN',\n",
    "                        'VEGAS':'TESCAN',\n",
    "                        'JEOL':'JEOL',\n",
    "                        'ZEISS':'Zeiss',\n",
    "                        'KYKY-EM3200':'KYKY',\n",
    "                        'SU8010':'Hitachi',\n",
    "                        'SU8000':'Hitachi',\n",
    "                        'SU3500':'Hitachi',\n",
    "                        'SU1510':'Hitachi',\n",
    "                        'SU8020':'Hitachi',\n",
    "                        'SU5000':'Hitachi',\n",
    "                        '\\$U8000':'Hitachi',\n",
    "                       '\\$U8020':'Hitachi',\n",
    "                       '\\$U8220':'Hitachi',\n",
    "                        'SU70':'Hitachi',\n",
    "                        'SU8220':'Hitachi',\n",
    "                        'S-3400N':'Hitachi',\n",
    "                        'S-4800':'Hitachi',\n",
    "                       'S3400N':'Hitachi',\n",
    "                        'Regulus':'Hitachi',\n",
    "                        'BSED':'FEI/Thermo',\n",
    "                        'Quanta':'FEI/Thermo',\n",
    "                       'QUANTA':'FEI/Thermo',\n",
    "                        'Nova':'FEI/Thermo',\n",
    "                        'NanoSEM':'FEI/Thermo',\n",
    "                       'InBeam':'TESCAN',\n",
    "                       'AIS2300C':'Seron',\n",
    "                       '2300C':'Seron',\n",
    "                       'NOVA':'FEI/Thermo',\n",
    "                       'GEMINI':'Zeiss',\n",
    "                       'Apreo':'FEI/Thermo'\n",
    "                       }\n",
    "\n",
    "    scope_model_dict = {'VEGA':'TESCAN VEGA',\n",
    "                       'MIRA':'TESCAN MIRA','GEMINI':'Zeiss Gemini'}\n",
    "\n",
    "    facility_dict = {'RMRC':'RMRC',\n",
    "                     'IITM':'IIT Mandi',\n",
    "                     'Mandi':'IIT Mandi',\n",
    "                    'IROST':'IROST',\n",
    "                    'IITB-ChE':'IIT Bombay',\n",
    "                    'SAIFIITB':'IIT Bombay',\n",
    "                    'pilani':'BITS Pilani',\n",
    "                     'Rural':'Gandhigram Rural Insitute',\n",
    "                     'Gandhigram':'Gandhigram Rural Insitute',\n",
    "                     'RAZI':'Razi Foundation',\n",
    "                     'Space':'Insitute of Space Technology',\n",
    "                     'USIF,':'USIF: Aligarh Muslim University',\n",
    "                     'SASTRA':'SASTRA',\n",
    "                     'NRC':'National Research Center', #how to cover 'National Research Center'?\n",
    "                     '(MUMS)':'Mashhad University',\n",
    "                     'SUT':'Suranaree University of Technology',\n",
    "                     'Patiala':'Thapar University',\n",
    "                     'METU':'Middle East Technical University',\n",
    "                     'ALIGARH':'USIF: Aligarh Muslim University',\n",
    "                     'ANNA':'Anna University',\n",
    "                     'UFSC':'LCME-UFSC',\n",
    "                     'CRNN':'Calcutta CRNN',\n",
    "                     '(CIFC)':'IIT BHU',\n",
    "                     'Amirkabir':'Amirkabir University'\n",
    "                    }\n",
    "\n",
    "    for key, value in scope_make_dict.items():\n",
    "        text_df.loc[text_df['text'].str.contains(key), 'instrument_make'] = value\n",
    "\n",
    "    instrument_use_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','instrument_make']).groupby('doi')\\\n",
    "    ['instrument_make'].apply(list).apply(lambda x: ', '.join(x))\n",
    "\n",
    "    n_instrument_use_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','instrument_make']).groupby('doi')\\\n",
    "    ['instrument_make'].apply(list).apply(lambda x: len(x))\n",
    "\n",
    "    text_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','text']).groupby('doi')\\\n",
    "    ['text'].apply(list).apply(lambda x: ', '.join(x))    \n",
    "\n",
    "    eval_df = pd.DataFrame(text_by_doi)\n",
    "    eval_df['instrument_make'] = instrument_use_by_doi\n",
    "    eval_df['n_instruments'] = n_instrument_use_by_doi\n",
    "\n",
    "    eval_df = eval_df.reset_index()\n",
    "\n",
    "    count = 0\n",
    "    full_text_array = []\n",
    "\n",
    "    for doi in eval_df['doi'].unique():\n",
    "        try:\n",
    "            url = 'https://api.elsevier.com/content/article/doi/' + doi + '?httpAccept=text/plain' + '&APIKey=' + api_key\n",
    "\n",
    "            response = requests.get(url, headers= requestHeader(url))\n",
    "\n",
    "            full_text_array.append(pd.DataFrame({'doi':[doi], 'full_text':[response.text]}))\n",
    "        except:\n",
    "            print('Error retrieving fulltext for DOI:' + doi)\n",
    "\n",
    "        count += 1\n",
    "        print(str(count) + ' of ' + str(eval_df['doi'].nunique()) + ' DOIs processed.')\n",
    "        if count % 10 == 0:\n",
    "            clear_output()  \n",
    "\n",
    "    full_text_df = pd.concat(full_text_array)\n",
    "\n",
    "    for index, row in full_text_df.iterrows():\n",
    "        filename = '../data/sem_full_text/' + row['doi'].replace('/','_') + '.txt'\n",
    "\n",
    "        with open(filename, 'w', encoding='utf') as f:\n",
    "            f.write(row['full_text'])\n",
    "\n",
    "    full_text_array = []\n",
    "\n",
    "    for index, row in eval_df.iterrows():\n",
    "        doi = row['doi']\n",
    "        filename = '../data/sem_full_text/' + row['doi'].replace('/','_') + '.txt'\n",
    "\n",
    "        with open(filename, 'r', encoding='utf') as f:\n",
    "            full_text = f.read()\n",
    "\n",
    "        full_text_array.append(pd.DataFrame({'doi':[doi], 'full_text':[full_text]}))\n",
    "\n",
    "    full_text_df = pd.concat(full_text_array)\n",
    "\n",
    "    brand_marks = ['TESCAN', 'TE-SCAN', 'TE SCAN', 'tesscan' \n",
    "                   'phillips', 'philips', 'jeol', \n",
    "                   'ky-ky', 'kyky', 'seron', \n",
    "                   'je-ol', 'zeiss', 'nova nano', 'tscan',\n",
    "                  'fei quanta', 'quanta fei', 'leica', \"philip's\",\n",
    "                   'joel', 'cambridge', 'Hithachi', \n",
    "                   'hitachi', 'hitashi', '1455VP', '960A', \n",
    "                   'XL30', 'XL-30', 'superscan', 'super scan', \n",
    "                   'Cam scan', 'camscan', 'MV2300', 'quanta', \n",
    "                   'fei', 'nanosem']\n",
    "\n",
    "    context_str_list = []\n",
    "    count = 0\n",
    "    for index, row in full_text_df.iterrows():\n",
    "        doi = row['doi']\n",
    "        full_text = row['full_text']\n",
    "        full_text_lower = full_text.lower()\n",
    "\n",
    "        context_str_array = []\n",
    "        match_array = []\n",
    "\n",
    "        # where references start\n",
    "        ref_match = re.search('References \\[1\\]', full_text)\n",
    "        if not ref_match:\n",
    "            ref_match = re.search('Reference \\[1\\]', full_text)\n",
    "        if not ref_match:\n",
    "            ref_match = re.search('  References   ', full_text)\n",
    "        if ref_match:\n",
    "            ref_start = ref_match.start()\n",
    "        else:\n",
    "            ref_match = 10_000_000\n",
    "\n",
    "        for brand_mark in brand_marks:\n",
    "            for match in re.finditer(brand_mark.lower(), full_text_lower):\n",
    "                if True:\n",
    "                    if (len(match_array) == 0):\n",
    "                        context_str_array.append(full_text[match.start()-150:match.start()+150])\n",
    "                        match_array.append(match.start())\n",
    "                    else:\n",
    "                        if (np.abs(np.array(match_array) - match.start()) > 150).all():\n",
    "                            context_str_array.append(full_text[match.start()-150:match.start()+150])\n",
    "                            match_array.append(match.start())\n",
    "        context_str = '... ...'.join(context_str_array)\n",
    "        context_str_list.append(context_str)\n",
    "\n",
    "        count += 1\n",
    "        print(str(count) + ' of ' + str(len(eval_df)) + ' DOIs processed.')\n",
    "        if count % 10 == 0:\n",
    "            clear_output()\n",
    "\n",
    "    context_df = pd.DataFrame({'doi': full_text_df['doi'].values, 'context_str':context_str_list})\n",
    "    context_df = pd.merge(eval_df, context_df)\n",
    "    context_df['doi'] = context_df['doi'].apply(lambda x: 'https://doi.org/' + x)\n",
    "\n",
    "    context_df.to_csv('../data/full_text_sem_images_' + journal_name + '_231217.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0672bf7",
   "metadata": {},
   "source": [
    "PLOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeeeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plos journals\n",
    "\n",
    "for journal_name in ['pone']:\n",
    "\n",
    "    text_df = pd.read_csv('../data/tesseract_ocr_sem_batch_231204_' + journal_name + '.csv')\n",
    "\n",
    "    meta_df = pd.read_csv('../data/sem_images_240125_batch.csv')\n",
    "\n",
    "    text_df = pd.merge(meta_df, text_df).copy()\n",
    "\n",
    "    text_df = text_df.dropna(subset=['text']).copy()\n",
    "\n",
    "    text_df['instrument_make'] = np.nan\n",
    "\n",
    "    scope_make_dict = {'TESCAN':'TESCAN',\n",
    "                        'TESCA':'TESCAN',\n",
    "                        'TESC':'TESCAN',\n",
    "                        'VEGA':'TESCAN',\n",
    "                        'VEGA3':'TESCAN',\n",
    "                        'VEGAN':'TESCAN',\n",
    "                        'VEGAS':'TESCAN',\n",
    "                        'JEOL':'JEOL',\n",
    "                        'ZEISS':'Zeiss',\n",
    "                        'KYKY-EM3200':'KYKY',\n",
    "                        'SU8010':'Hitachi',\n",
    "                        'SU8000':'Hitachi',\n",
    "                        'SU3500':'Hitachi',\n",
    "                        'SU1510':'Hitachi',\n",
    "                        'SU8020':'Hitachi',\n",
    "                        'SU5000':'Hitachi',\n",
    "                        '\\$U8000':'Hitachi',\n",
    "                       '\\$U8020':'Hitachi',\n",
    "                       '\\$U8220':'Hitachi',\n",
    "                        'SU70':'Hitachi',\n",
    "                        'SU8220':'Hitachi',\n",
    "                        'S-3400N':'Hitachi',\n",
    "                        'S-4800':'Hitachi',\n",
    "                       'S3400N':'Hitachi',\n",
    "                        'Regulus':'Hitachi',\n",
    "                        'BSED':'FEI/Thermo',\n",
    "                        'Quanta':'FEI/Thermo',\n",
    "                       'QUANTA':'FEI/Thermo',\n",
    "                        'Nova':'FEI/Thermo',\n",
    "                        'NanoSEM':'FEI/Thermo',\n",
    "                       'InBeam':'TESCAN',\n",
    "                       'AIS2300C':'Seron',\n",
    "                       '2300C':'Seron',\n",
    "                       'NOVA':'FEI/Thermo',\n",
    "                       'GEMINI':'Zeiss',\n",
    "                       'Apreo':'FEI/Thermo'\n",
    "                       }\n",
    "\n",
    "    scope_model_dict = {'VEGA':'TESCAN VEGA',\n",
    "                       'MIRA':'TESCAN MIRA','GEMINI':'Zeiss Gemini'}\n",
    "\n",
    "    facility_dict = {'RMRC':'RMRC',\n",
    "                     'IITM':'IIT Mandi',\n",
    "                     'Mandi':'IIT Mandi',\n",
    "                    'IROST':'IROST',\n",
    "                    'IITB-ChE':'IIT Bombay',\n",
    "                    'SAIFIITB':'IIT Bombay',\n",
    "                    'pilani':'BITS Pilani',\n",
    "                     'Rural':'Gandhigram Rural Insitute',\n",
    "                     'Gandhigram':'Gandhigram Rural Insitute',\n",
    "                     'RAZI':'Razi Foundation',\n",
    "                     'Space':'Insitute of Space Technology',\n",
    "                     'USIF,':'USIF: Aligarh Muslim University',\n",
    "                     'SASTRA':'SASTRA',\n",
    "                     'NRC':'National Research Center', #how to cover 'National Research Center'?\n",
    "                     '(MUMS)':'Mashhad University',\n",
    "                     'SUT':'Suranaree University of Technology',\n",
    "                     'Patiala':'Thapar University',\n",
    "                     'METU':'Middle East Technical University',\n",
    "                     'ALIGARH':'USIF: Aligarh Muslim University',\n",
    "                     'ANNA':'Anna University',\n",
    "                     'UFSC':'LCME-UFSC',\n",
    "                     'CRNN':'Calcutta CRNN',\n",
    "                     '(CIFC)':'IIT BHU',\n",
    "                     'Amirkabir':'Amirkabir University'\n",
    "                    }\n",
    "\n",
    "    for key, value in scope_make_dict.items():\n",
    "        text_df.loc[text_df['text'].str.contains(key), 'instrument_make'] = value\n",
    "\n",
    "    instrument_use_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','instrument_make']).groupby('doi')\\\n",
    "    ['instrument_make'].apply(list).apply(lambda x: ', '.join(x))\n",
    "\n",
    "    n_instrument_use_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','instrument_make']).groupby('doi')\\\n",
    "    ['instrument_make'].apply(list).apply(lambda x: len(x))\n",
    "\n",
    "    text_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','text']).groupby('doi')\\\n",
    "    ['text'].apply(list).apply(lambda x: ', '.join(x))    \n",
    "\n",
    "    eval_df = pd.DataFrame(text_by_doi)\n",
    "    eval_df['instrument_make'] = instrument_use_by_doi\n",
    "    eval_df['n_instruments'] = n_instrument_use_by_doi\n",
    "\n",
    "    eval_df = eval_df.reset_index()\n",
    "\n",
    "    count = 0\n",
    "    full_text_array = []\n",
    "\n",
    "    for doi in eval_df['doi'].unique():\n",
    "        try:\n",
    "            url = 'https://doi.org/' + doi\n",
    "\n",
    "            response = requests.get(url, headers= requestHeader(url))\n",
    "        \n",
    "            file = BeautifulSoup(response.text, \"lxml\")\n",
    "            \n",
    "            full_text = ' '.join([p.text for p in file.find_all('p')])\n",
    "            \n",
    "            full_text_array.append(pd.DataFrame({'doi':[doi], 'full_text':[full_text]}))\n",
    "            \n",
    "            file.decompose()\n",
    "            \n",
    "        except:\n",
    "            print('Error retrieving fulltext for DOI:' + doi)\n",
    "\n",
    "        count += 1\n",
    "        print(str(count) + ' of ' + str(eval_df['doi'].nunique()) + ' DOIs processed.')\n",
    "        if count % 10 == 0:\n",
    "            clear_output()  \n",
    "\n",
    "    full_text_df = pd.concat(full_text_array)\n",
    "\n",
    "    for index, row in full_text_df.iterrows():\n",
    "        filename = '../data/sem_full_text/' + row['doi'].replace('/','_') + '.txt'\n",
    "\n",
    "        with open(filename, 'w', encoding='utf') as f:\n",
    "            f.write(row['full_text'])\n",
    "\n",
    "    full_text_array = []\n",
    "\n",
    "    for index, row in eval_df.iterrows():\n",
    "        doi = row['doi']\n",
    "        filename = '../data/sem_full_text/' + row['doi'].replace('/','_') + '.txt'\n",
    "\n",
    "        with open(filename, 'r', encoding='utf') as f:\n",
    "            full_text = f.read()\n",
    "\n",
    "        full_text_array.append(pd.DataFrame({'doi':[doi], 'full_text':[full_text]}))\n",
    "\n",
    "    full_text_df = pd.concat(full_text_array)\n",
    "\n",
    "    brand_marks = ['TESCAN', 'TE-SCAN', 'TE SCAN', 'tesscan' \n",
    "                   'phillips', 'philips', 'jeol', \n",
    "                   'ky-ky', 'kyky', 'seron', \n",
    "                   'je-ol', 'zeiss', 'nova nano', 'tscan',\n",
    "                  'fei quanta', 'quanta fei', 'leica', \"philip's\",\n",
    "                   'joel', 'cambridge', 'Hithachi', \n",
    "                   'hitachi', 'hitashi', '1455VP', '960A', \n",
    "                   'XL30', 'XL-30', 'superscan', 'super scan', \n",
    "                   'Cam scan', 'camscan', 'MV2300', 'quanta', \n",
    "                   'fei', 'nanosem']\n",
    "\n",
    "    context_str_list = []\n",
    "    count = 0\n",
    "    for index, row in full_text_df.iterrows():\n",
    "        doi = row['doi']\n",
    "        full_text = row['full_text']\n",
    "        full_text_lower = full_text.lower()\n",
    "\n",
    "        context_str_array = []\n",
    "        match_array = []\n",
    "\n",
    "        # where references start\n",
    "        ref_match = re.search('References \\[1\\]', full_text)\n",
    "        if not ref_match:\n",
    "            ref_match = re.search('Reference \\[1\\]', full_text)\n",
    "        if not ref_match:\n",
    "            ref_match = re.search('  References   ', full_text)\n",
    "        if ref_match:\n",
    "            ref_start = ref_match.start()\n",
    "        else:\n",
    "            ref_match = 10_000_000\n",
    "\n",
    "        for brand_mark in brand_marks:\n",
    "            for match in re.finditer(brand_mark.lower(), full_text_lower):\n",
    "                if True:\n",
    "                    if (len(match_array) == 0):\n",
    "                        context_str_array.append(full_text[match.start()-150:match.start()+150])\n",
    "                        match_array.append(match.start())\n",
    "                    else:\n",
    "                        if (np.abs(np.array(match_array) - match.start()) > 150).all():\n",
    "                            context_str_array.append(full_text[match.start()-150:match.start()+150])\n",
    "                            match_array.append(match.start())\n",
    "        context_str = '... ...'.join(context_str_array)\n",
    "        context_str_list.append(context_str)\n",
    "\n",
    "        count += 1\n",
    "        print(str(count) + ' of ' + str(len(eval_df)) + ' DOIs processed.')\n",
    "        if count % 10 == 0:\n",
    "            clear_output()\n",
    "\n",
    "    context_df = pd.DataFrame({'doi': full_text_df['doi'].values, 'context_str':context_str_list})\n",
    "    context_df = pd.merge(eval_df, context_df)\n",
    "    context_df['doi'] = context_df['doi'].apply(lambda x: 'https://doi.org/' + x)\n",
    "\n",
    "    context_df.to_csv('../data/full_text_sem_images_' + journal_name + '_240104.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d6fa3e",
   "metadata": {},
   "source": [
    "Frontiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49614f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontiers journals\n",
    "\n",
    "for journal_name in ['fchem']:\n",
    "\n",
    "    text_df = pd.read_csv('../data/tesseract_ocr_sem_batch_240104_' + journal_name + '.csv')\n",
    "\n",
    "    meta_df = pd.read_csv('../data/sem_images_240104_batch.csv')\n",
    "\n",
    "    text_df = pd.merge(meta_df, text_df).copy()\n",
    "\n",
    "    text_df = text_df.dropna(subset=['text']).copy()\n",
    "\n",
    "    text_df['instrument_make'] = np.nan\n",
    "\n",
    "    scope_make_dict = {'TESCAN':'TESCAN',\n",
    "                        'TESCA':'TESCAN',\n",
    "                        'TESC':'TESCAN',\n",
    "                        'VEGA':'TESCAN',\n",
    "                        'VEGA3':'TESCAN',\n",
    "                        'VEGAN':'TESCAN',\n",
    "                        'VEGAS':'TESCAN',\n",
    "                        'JEOL':'JEOL',\n",
    "                        'ZEISS':'Zeiss',\n",
    "                        'KYKY-EM3200':'KYKY',\n",
    "                        'SU8010':'Hitachi',\n",
    "                        'SU8000':'Hitachi',\n",
    "                        'SU3500':'Hitachi',\n",
    "                        'SU1510':'Hitachi',\n",
    "                        'SU8020':'Hitachi',\n",
    "                        'SU5000':'Hitachi',\n",
    "                        '\\$U8000':'Hitachi',\n",
    "                       '\\$U8020':'Hitachi',\n",
    "                       '\\$U8220':'Hitachi',\n",
    "                        'SU70':'Hitachi',\n",
    "                        'SU8220':'Hitachi',\n",
    "                        'S-3400N':'Hitachi',\n",
    "                        'S-4800':'Hitachi',\n",
    "                       'S3400N':'Hitachi',\n",
    "                        'Regulus':'Hitachi',\n",
    "                        'BSED':'FEI/Thermo',\n",
    "                        'Quanta':'FEI/Thermo',\n",
    "                       'QUANTA':'FEI/Thermo',\n",
    "                        'Nova':'FEI/Thermo',\n",
    "                        'NanoSEM':'FEI/Thermo',\n",
    "                       'InBeam':'TESCAN',\n",
    "                       'AIS2300C':'Seron',\n",
    "                       '2300C':'Seron',\n",
    "                       'NOVA':'FEI/Thermo',\n",
    "                       'GEMINI':'Zeiss',\n",
    "                       'Apreo':'FEI/Thermo'\n",
    "                       }\n",
    "\n",
    "    scope_model_dict = {'VEGA':'TESCAN VEGA',\n",
    "                       'MIRA':'TESCAN MIRA','GEMINI':'Zeiss Gemini'}\n",
    "\n",
    "    facility_dict = {'RMRC':'RMRC',\n",
    "                     'IITM':'IIT Mandi',\n",
    "                     'Mandi':'IIT Mandi',\n",
    "                    'IROST':'IROST',\n",
    "                    'IITB-ChE':'IIT Bombay',\n",
    "                    'SAIFIITB':'IIT Bombay',\n",
    "                    'pilani':'BITS Pilani',\n",
    "                     'Rural':'Gandhigram Rural Insitute',\n",
    "                     'Gandhigram':'Gandhigram Rural Insitute',\n",
    "                     'RAZI':'Razi Foundation',\n",
    "                     'Space':'Insitute of Space Technology',\n",
    "                     'USIF,':'USIF: Aligarh Muslim University',\n",
    "                     'SASTRA':'SASTRA',\n",
    "                     'NRC':'National Research Center', #how to cover 'National Research Center'?\n",
    "                     '(MUMS)':'Mashhad University',\n",
    "                     'SUT':'Suranaree University of Technology',\n",
    "                     'Patiala':'Thapar University',\n",
    "                     'METU':'Middle East Technical University',\n",
    "                     'ALIGARH':'USIF: Aligarh Muslim University',\n",
    "                     'ANNA':'Anna University',\n",
    "                     'UFSC':'LCME-UFSC',\n",
    "                     'CRNN':'Calcutta CRNN',\n",
    "                     '(CIFC)':'IIT BHU',\n",
    "                     'Amirkabir':'Amirkabir University'\n",
    "                    }\n",
    "\n",
    "    for key, value in scope_make_dict.items():\n",
    "        text_df.loc[text_df['text'].str.contains(key), 'instrument_make'] = value\n",
    "\n",
    "    instrument_use_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','instrument_make']).groupby('doi')\\\n",
    "    ['instrument_make'].apply(list).apply(lambda x: ', '.join(x))\n",
    "\n",
    "    n_instrument_use_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','instrument_make']).groupby('doi')\\\n",
    "    ['instrument_make'].apply(list).apply(lambda x: len(x))\n",
    "\n",
    "    text_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','text']).groupby('doi')\\\n",
    "    ['text'].apply(list).apply(lambda x: ', '.join(x))    \n",
    "\n",
    "    eval_df = pd.DataFrame(text_by_doi)\n",
    "    eval_df['instrument_make'] = instrument_use_by_doi\n",
    "    eval_df['n_instruments'] = n_instrument_use_by_doi\n",
    "\n",
    "    eval_df = eval_df.reset_index()\n",
    "\n",
    "    count = 0\n",
    "    full_text_array = []\n",
    "\n",
    "    for doi in eval_df['doi'].unique():\n",
    "        try:\n",
    "            url = 'https://doi.org/' + doi\n",
    "\n",
    "            response = requests.get(url, headers= requestHeader(url))\n",
    "        \n",
    "            file = BeautifulSoup(response.text, \"lxml\")\n",
    "            \n",
    "            full_text = ' '.join([p.text for p in file.find_all('p')])\n",
    "            \n",
    "            full_text_array.append(pd.DataFrame({'doi':[doi], 'full_text':[full_text]}))\n",
    "            \n",
    "            file.decompose()\n",
    "            \n",
    "        except:\n",
    "            print('Error retrieving fulltext for DOI:' + doi)\n",
    "\n",
    "        count += 1\n",
    "        print(str(count) + ' of ' + str(eval_df['doi'].nunique()) + ' DOIs processed.')\n",
    "        if count % 10 == 0:\n",
    "            clear_output()  \n",
    "\n",
    "    full_text_df = pd.concat(full_text_array)\n",
    "\n",
    "    for index, row in full_text_df.iterrows():\n",
    "        filename = '../data/sem_full_text/' + row['doi'].replace('/','_') + '.txt'\n",
    "\n",
    "        with open(filename, 'w', encoding='utf') as f:\n",
    "            f.write(row['full_text'])\n",
    "\n",
    "    full_text_array = []\n",
    "\n",
    "    for index, row in eval_df.iterrows():\n",
    "        doi = row['doi']\n",
    "        filename = '../data/sem_full_text/' + row['doi'].replace('/','_') + '.txt'\n",
    "\n",
    "        with open(filename, 'r', encoding='utf') as f:\n",
    "            full_text = f.read()\n",
    "\n",
    "        full_text_array.append(pd.DataFrame({'doi':[doi], 'full_text':[full_text]}))\n",
    "\n",
    "    full_text_df = pd.concat(full_text_array)\n",
    "\n",
    "    brand_marks = ['TESCAN', 'TE-SCAN', 'TE SCAN', 'tesscan' \n",
    "                   'phillips', 'philips', 'jeol', \n",
    "                   'ky-ky', 'kyky', 'seron', \n",
    "                   'je-ol', 'zeiss', 'nova nano', 'tscan',\n",
    "                  'fei quanta', 'quanta fei', 'leica', \"philip's\",\n",
    "                   'joel', 'cambridge', 'Hithachi', \n",
    "                   'hitachi', 'hitashi', '1455VP', '960A', \n",
    "                   'XL30', 'XL-30', 'superscan', 'super scan', \n",
    "                   'Cam scan', 'camscan', 'MV2300', 'quanta', \n",
    "                   'fei', 'nanosem']\n",
    "\n",
    "    context_str_list = []\n",
    "    count = 0\n",
    "    for index, row in full_text_df.iterrows():\n",
    "        doi = row['doi']\n",
    "        full_text = row['full_text']\n",
    "        full_text_lower = full_text.lower()\n",
    "\n",
    "        context_str_array = []\n",
    "        match_array = []\n",
    "\n",
    "        # where references start\n",
    "        ref_match = re.search('References \\[1\\]', full_text)\n",
    "        if not ref_match:\n",
    "            ref_match = re.search('Reference \\[1\\]', full_text)\n",
    "        if not ref_match:\n",
    "            ref_match = re.search('  References   ', full_text)\n",
    "        if ref_match:\n",
    "            ref_start = ref_match.start()\n",
    "        else:\n",
    "            ref_match = 10_000_000\n",
    "\n",
    "        for brand_mark in brand_marks:\n",
    "            for match in re.finditer(brand_mark.lower(), full_text_lower):\n",
    "                if True:\n",
    "                    if (len(match_array) == 0):\n",
    "                        context_str_array.append(full_text[match.start()-150:match.start()+150])\n",
    "                        match_array.append(match.start())\n",
    "                    else:\n",
    "                        if (np.abs(np.array(match_array) - match.start()) > 150).all():\n",
    "                            context_str_array.append(full_text[match.start()-150:match.start()+150])\n",
    "                            match_array.append(match.start())\n",
    "        context_str = '... ...'.join(context_str_array)\n",
    "        context_str_list.append(context_str)\n",
    "\n",
    "        count += 1\n",
    "        print(str(count) + ' of ' + str(len(eval_df)) + ' DOIs processed.')\n",
    "        if count % 10 == 0:\n",
    "            clear_output()\n",
    "\n",
    "    context_df = pd.DataFrame({'doi': full_text_df['doi'].values, 'context_str':context_str_list})\n",
    "    context_df = pd.merge(eval_df, context_df)\n",
    "    context_df['doi'] = context_df['doi'].apply(lambda x: 'https://doi.org/' + x)\n",
    "\n",
    "    context_df.to_csv('../data/full_text_sem_images_' + journal_name + '_240104.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d6db7",
   "metadata": {},
   "source": [
    "Nature portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c606f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nature portfolio journals\n",
    "\n",
    "for journal_name in ['srep']:\n",
    "\n",
    "    text_df = pd.read_csv('../data/tesseract_ocr_sem_batch_231227_' + journal_name + '.csv')\n",
    "\n",
    "    meta_df = pd.read_csv('../data/sem_images_231227_batch.csv')\n",
    "\n",
    "    text_df = pd.merge(meta_df, text_df).copy()\n",
    "\n",
    "    text_df = text_df.dropna(subset=['text']).copy()\n",
    "\n",
    "    text_df['instrument_make'] = np.nan\n",
    "\n",
    "    scope_make_dict = {'TESCAN':'TESCAN',\n",
    "                        'TESCA':'TESCAN',\n",
    "                        'TESC':'TESCAN',\n",
    "                        'VEGA':'TESCAN',\n",
    "                        'VEGA3':'TESCAN',\n",
    "                        'VEGAN':'TESCAN',\n",
    "                        'VEGAS':'TESCAN',\n",
    "                        'JEOL':'JEOL',\n",
    "                        'ZEISS':'Zeiss',\n",
    "                        'KYKY-EM3200':'KYKY',\n",
    "                        'SU8010':'Hitachi',\n",
    "                        'SU8000':'Hitachi',\n",
    "                        'SU3500':'Hitachi',\n",
    "                        'SU1510':'Hitachi',\n",
    "                        'SU8020':'Hitachi',\n",
    "                        'SU5000':'Hitachi',\n",
    "                        '\\$U8000':'Hitachi',\n",
    "                       '\\$U8020':'Hitachi',\n",
    "                       '\\$U8220':'Hitachi',\n",
    "                        'SU70':'Hitachi',\n",
    "                        'SU8220':'Hitachi',\n",
    "                        'S-3400N':'Hitachi',\n",
    "                        'S-4800':'Hitachi',\n",
    "                       'S3400N':'Hitachi',\n",
    "                        'Regulus':'Hitachi',\n",
    "                        'BSED':'FEI/Thermo',\n",
    "                        'Quanta':'FEI/Thermo',\n",
    "                       'QUANTA':'FEI/Thermo',\n",
    "                        'Nova':'FEI/Thermo',\n",
    "                        'NanoSEM':'FEI/Thermo',\n",
    "                       'InBeam':'TESCAN',\n",
    "                       'AIS2300C':'Seron',\n",
    "                       '2300C':'Seron',\n",
    "                       'NOVA':'FEI/Thermo',\n",
    "                       'GEMINI':'Zeiss',\n",
    "                       'Apreo':'FEI/Thermo'\n",
    "                       }\n",
    "\n",
    "    scope_model_dict = {'VEGA':'TESCAN VEGA',\n",
    "                       'MIRA':'TESCAN MIRA','GEMINI':'Zeiss Gemini'}\n",
    "\n",
    "    facility_dict = {'RMRC':'RMRC',\n",
    "                     'IITM':'IIT Mandi',\n",
    "                     'Mandi':'IIT Mandi',\n",
    "                    'IROST':'IROST',\n",
    "                    'IITB-ChE':'IIT Bombay',\n",
    "                    'SAIFIITB':'IIT Bombay',\n",
    "                    'pilani':'BITS Pilani',\n",
    "                     'Rural':'Gandhigram Rural Insitute',\n",
    "                     'Gandhigram':'Gandhigram Rural Insitute',\n",
    "                     'RAZI':'Razi Foundation',\n",
    "                     'Space':'Insitute of Space Technology',\n",
    "                     'USIF,':'USIF: Aligarh Muslim University',\n",
    "                     'SASTRA':'SASTRA',\n",
    "                     'NRC':'National Research Center', #how to cover 'National Research Center'?\n",
    "                     '(MUMS)':'Mashhad University',\n",
    "                     'SUT':'Suranaree University of Technology',\n",
    "                     'Patiala':'Thapar University',\n",
    "                     'METU':'Middle East Technical University',\n",
    "                     'ALIGARH':'USIF: Aligarh Muslim University',\n",
    "                     'ANNA':'Anna University',\n",
    "                     'UFSC':'LCME-UFSC',\n",
    "                     'CRNN':'Calcutta CRNN',\n",
    "                     '(CIFC)':'IIT BHU',\n",
    "                     'Amirkabir':'Amirkabir University'\n",
    "                    }\n",
    "\n",
    "    for key, value in scope_make_dict.items():\n",
    "        text_df.loc[text_df['text'].str.contains(key), 'instrument_make'] = value\n",
    "\n",
    "    instrument_use_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','instrument_make']).groupby('doi')\\\n",
    "    ['instrument_make'].apply(list).apply(lambda x: ', '.join(x))\n",
    "\n",
    "    n_instrument_use_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','instrument_make']).groupby('doi')\\\n",
    "    ['instrument_make'].apply(list).apply(lambda x: len(x))\n",
    "\n",
    "    text_by_doi = text_df[~text_df['instrument_make'].isna()]\\\n",
    "    .drop_duplicates(subset=['doi','text']).groupby('doi')\\\n",
    "    ['text'].apply(list).apply(lambda x: ', '.join(x))    \n",
    "\n",
    "    eval_df = pd.DataFrame(text_by_doi)\n",
    "    eval_df['instrument_make'] = instrument_use_by_doi\n",
    "    eval_df['n_instruments'] = n_instrument_use_by_doi\n",
    "\n",
    "    eval_df = eval_df.reset_index()\n",
    "\n",
    "    count = 0\n",
    "    full_text_array = []\n",
    "\n",
    "    for doi in eval_df['doi'].unique():\n",
    "        try:\n",
    "            url = 'https://doi.org/' + doi\n",
    "\n",
    "            response = requests.get(url, headers= requestHeader(url))\n",
    "        \n",
    "            file = BeautifulSoup(response.text, \"lxml\")\n",
    "            \n",
    "            full_text = ' '.join([p.text for p in file.find_all('p')])\n",
    "            \n",
    "            full_text_array.append(pd.DataFrame({'doi':[doi], 'full_text':[full_text]}))\n",
    "            \n",
    "            file.decompose()\n",
    "            \n",
    "        except:\n",
    "            print('Error retrieving fulltext for DOI:' + doi)\n",
    "\n",
    "        count += 1\n",
    "        print(str(count) + ' of ' + str(eval_df['doi'].nunique()) + ' DOIs processed.')\n",
    "        if count % 10 == 0:\n",
    "            clear_output()  \n",
    "\n",
    "    full_text_df = pd.concat(full_text_array)\n",
    "\n",
    "    for index, row in full_text_df.iterrows():\n",
    "        filename = '../data/sem_full_text/' + row['doi'].replace('/','_') + '.txt'\n",
    "\n",
    "        with open(filename, 'w', encoding='utf') as f:\n",
    "            f.write(row['full_text'])\n",
    "\n",
    "    full_text_array = []\n",
    "\n",
    "    for index, row in eval_df.iterrows():\n",
    "        doi = row['doi']\n",
    "        filename = '../data/sem_full_text/' + row['doi'].replace('/','_') + '.txt'\n",
    "\n",
    "        with open(filename, 'r', encoding='utf') as f:\n",
    "            full_text = f.read()\n",
    "\n",
    "        full_text_array.append(pd.DataFrame({'doi':[doi], 'full_text':[full_text]}))\n",
    "\n",
    "    full_text_df = pd.concat(full_text_array)\n",
    "\n",
    "    brand_marks = ['TESCAN', 'TE-SCAN', 'TE SCAN', 'tesscan' \n",
    "                   'phillips', 'philips', 'jeol', \n",
    "                   'ky-ky', 'kyky', 'seron', \n",
    "                   'je-ol', 'zeiss', 'nova nano', 'tscan',\n",
    "                  'fei quanta', 'quanta fei', 'leica', \"philip's\",\n",
    "                   'joel', 'cambridge', 'Hithachi', \n",
    "                   'hitachi', 'hitashi', '1455VP', '960A', \n",
    "                   'XL30', 'XL-30', 'superscan', 'super scan', \n",
    "                   'Cam scan', 'camscan', 'MV2300', 'quanta', \n",
    "                   'fei', 'nanosem']\n",
    "\n",
    "    context_str_list = []\n",
    "    count = 0\n",
    "    for index, row in full_text_df.iterrows():\n",
    "        doi = row['doi']\n",
    "        full_text = row['full_text']\n",
    "        full_text_lower = full_text.lower()\n",
    "\n",
    "        context_str_array = []\n",
    "        match_array = []\n",
    "\n",
    "        # where references start\n",
    "        ref_match = re.search('References \\[1\\]', full_text)\n",
    "        if not ref_match:\n",
    "            ref_match = re.search('Reference \\[1\\]', full_text)\n",
    "        if not ref_match:\n",
    "            ref_match = re.search('  References   ', full_text)\n",
    "        if ref_match:\n",
    "            ref_start = ref_match.start()\n",
    "        else:\n",
    "            ref_match = 10_000_000\n",
    "\n",
    "        for brand_mark in brand_marks:\n",
    "            for match in re.finditer(brand_mark.lower(), full_text_lower):\n",
    "                if True:\n",
    "                    if (len(match_array) == 0):\n",
    "                        context_str_array.append(full_text[match.start()-150:match.start()+150])\n",
    "                        match_array.append(match.start())\n",
    "                    else:\n",
    "                        if (np.abs(np.array(match_array) - match.start()) > 150).all():\n",
    "                            context_str_array.append(full_text[match.start()-150:match.start()+150])\n",
    "                            match_array.append(match.start())\n",
    "        context_str = '... ...'.join(context_str_array)\n",
    "        context_str_list.append(context_str)\n",
    "\n",
    "        count += 1\n",
    "        print(str(count) + ' of ' + str(len(eval_df)) + ' DOIs processed.')\n",
    "        if count % 10 == 0:\n",
    "            clear_output()\n",
    "\n",
    "    context_df = pd.DataFrame({'doi': full_text_df['doi'].values, 'context_str':context_str_list})\n",
    "    context_df = pd.merge(eval_df, context_df)\n",
    "    context_df['doi'] = context_df['doi'].apply(lambda x: 'https://doi.org/' + x)\n",
    "\n",
    "    context_df.to_csv('../data/full_text_sem_images_' + journal_name + '_231227.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
